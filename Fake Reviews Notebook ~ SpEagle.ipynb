{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font face=\"Times New Roman\" style=\"text-align: center;\">\n",
    "        <h1>Beyond Reviews</h1>\n",
    "        <h2>Validating online consumer reviews using unsupervised machine learning methods.</h2>\n",
    "        <p><strong>Date:</strong> June 15, 2024</p>\n",
    "        <p><strong>Student:</strong> Maurits Christiaan Graaf</p>\n",
    "        <p><strong>Studentnumber:</strong> 660509</p>\n",
    "        <p><strong>Supervisor:</strong> Dr. D.J. (David) Kusterer</p>\n",
    "        <p><strong>Second Reader:</strong> Dr. M. (Maciej) Szymanowski</p>\n",
    "        <p><strong>Department:</strong> Marketing</p>\n",
    "        <p><strong>University:</strong> Rotterdam School of Management</p>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Importing Packages\n",
    "This codeblock imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.spatial.distance import cosine\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Import the dataset created and prepared in 'Fake reviews notebook ~ data preparation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149880, 48)\n",
      "Index(['Review_Rating', 'Review_Title', 'Review_Text', 'Review_Images',\n",
      "       'Product_ASIN', 'Parent_Product_ASIN', 'User_ID', 'Review_Timestamp',\n",
      "       'Helpful_Votes', 'Verified_Purchase', 'Main_Category', 'Product_Title',\n",
      "       'Product_Average_Rating', 'Count_product_ratings', 'Product_Features',\n",
      "       'Product_Description', 'Product_Price', 'Product_Images',\n",
      "       'Product_Videos', 'Product_Store', 'Product_Categories',\n",
      "       'Product_Details', 'Products_Bought_Together', 'Product_Subtitle',\n",
      "       'Product_Author', 'overall_category', 'Review_Title_Length',\n",
      "       'Review_Text_Length', 'Rating_Difference', 'Review_Rank',\n",
      "       'Extreme_Rating', 'Average_Text_Length', 'Text_Length_Difference',\n",
      "       'Review_Count', 'Singular_Review', 'Review_Words', 'Review_Word_Count',\n",
      "       'Fully_Capitalized_Words_Count', 'Fully_Capitalized_Words_Proportion',\n",
      "       'Capital_Letters_Excluding_Start_Count',\n",
      "       'Capital_Letters_Excluding_Start_Percentage',\n",
      "       'First_Person_Pronouns_Count', 'First_Person_Pronouns_Ratio',\n",
      "       'Exclamation_Sentence_Ratio', 'Sentiment_Scores', 'Cosine_Similarity',\n",
      "       'Review_Month', 'Review_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read from pickle (pkl)\n",
    "df = pd.read_pickle('concatenated_df.pkl')\n",
    "\n",
    "#Check whether import functioned correctly\n",
    "print(df.shape)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the SpEagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Feature Extraction ------ #####\n",
    "# Compute the maximum number of reviews per day\n",
    "df['Review_Date'] = pd.to_datetime(df['Review_Timestamp'])\n",
    "df['Review_ID'] = df.index if 'Review_ID' not in df.columns else df['Review_ID']\n",
    "df['Reviews_Per_Day'] = df.groupby(['User_ID', df['Review_Date'].dt.date])['Review_ID'].transform('count')\n",
    "df['Max_Reviews_Per_Day'] = df.groupby('User_ID')['Reviews_Per_Day'].transform('max')\n",
    "df['MNR'] = MinMaxScaler().fit_transform(df[['Max_Reviews_Per_Day']])\n",
    "\n",
    "# Compute reviewing burstiness\n",
    "tau = 28\n",
    "df['First_Review_Date'] = df.groupby('User_ID')['Review_Date'].transform('min')\n",
    "df['Last_Review_Date'] = df.groupby('User_ID')['Review_Date'].transform('max')\n",
    "df['Activity_Span'] = (df['Last_Review_Date'] - df['First_Review_Date']).dt.days\n",
    "df['BST'] = 1 - (df['Activity_Span'] / tau).clip(upper=1)\n",
    "\n",
    "# Compute the ratio of first reviews\n",
    "df['Is_First_Review'] = df.groupby('Product_ASIN')['Review_Timestamp'].rank(method='first') == 1\n",
    "df['First_Reviews'] = df.groupby('User_ID')['Is_First_Review'].transform('sum')\n",
    "df['Total_Reviews'] = df.groupby('User_ID')['Review_ID'].transform('count')\n",
    "df['RFR'] = df['First_Reviews'] / df['Total_Reviews']\n",
    "\n",
    "# Add a column for duplicate/near-duplicate based on cosine similarity\n",
    "df['duplicate'] = (df['Cosine_Similarity'] >= 0.9).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Graph Construction ------ #####\n",
    "# Constructing the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(row['User_ID'], type='user')\n",
    "    G.add_node(row['Review_ID'], type='review')\n",
    "    G.add_node(row['Product_ASIN'], type='product')\n",
    "    G.add_edge(row['User_ID'], row['Review_ID'], relation='writes')\n",
    "    G.add_edge(row['Review_ID'], row['Product_ASIN'], relation='belongs_to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Prior Estimation ------ #####\n",
    "# Function to estimate priors using all relevant features\n",
    "def estimate_priors(df):\n",
    "    user_features = df[['MNR', 'BST', 'RFR', 'Review_Count', 'Average_Text_Length', \n",
    "                        'First_Person_Pronouns_Ratio', 'Exclamation_Sentence_Ratio']].to_numpy()\n",
    "    review_features = df[['duplicate', 'Cosine_Similarity', 'Extreme_Rating', \n",
    "                          'Rating_Difference', 'Sentiment_Scores']].to_numpy()\n",
    "    user_priors = {user: np.mean(user_features[idx]) for idx, user in enumerate(df['User_ID'].unique())}\n",
    "    review_priors = {review: np.mean(review_features[idx]) for idx, review in enumerate(df['Review_ID'].unique())}\n",
    "    return user_priors, review_priors\n",
    "\n",
    "user_priors, review_priors = estimate_priors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Loopy Belief Propagation ------ #####\n",
    "def loopy_belief_propagation(G, user_priors, review_priors, max_iter=100, epsilon=1e-3):\n",
    "    # Initialize messages\n",
    "    messages = {edge: 1.0 for edge in G.edges}\n",
    "    prev_messages = messages.copy()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for edge in G.edges:\n",
    "            node_u, node_v = edge\n",
    "            if G.nodes[node_u]['type'] == 'user' and G.nodes[node_v]['type'] == 'review':\n",
    "                messages[edge] = user_priors[node_u] * review_priors[node_v]\n",
    "            elif G.nodes[node_u]['type'] == 'review' and G.nodes[node_v]['type'] == 'product':\n",
    "                messages[edge] = review_priors[node_u]\n",
    "        \n",
    "        # Check for convergence\n",
    "        delta = sum(abs(messages[edge] - prev_messages[edge]) for edge in G.edges)\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "        prev_messages = messages.copy()\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Run LBP\n",
    "messages = loopy_belief_propagation(G, user_priors, review_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Final Class Probabilities ------ #####\n",
    "# Assign class probabilities\n",
    "for node in G.nodes:\n",
    "    if G.nodes[node]['type'] == 'user':\n",
    "        G.nodes[node]['spam_prob'] = user_priors[node]\n",
    "    elif G.nodes[node]['type'] == 'review':\n",
    "        G.nodes[node]['spam_prob'] = review_priors[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "user_spam_prob = {node: G.nodes[node]['spam_prob'] for node in G.nodes if G.nodes[node]['type'] == 'user'}\n",
    "review_spam_prob = {node: G.nodes[node]['spam_prob'] for node in G.nodes if G.nodes[node]['type'] == 'review'}\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_spam_df = pd.DataFrame.from_dict(user_spam_prob, orient='index', columns=['User_Spam_Probability'])\n",
    "review_spam_df = pd.DataFrame.from_dict(review_spam_prob, orient='index', columns=['Review_Spam_Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ------ Merge and Save Combined DataFrame ------ #####\n",
    "# Merge spam probabilities with the original DataFrame\n",
    "df = df.merge(user_spam_df, left_on='User_ID', right_index=True, how='left')\n",
    "df = df.merge(review_spam_df, left_on='Review_ID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Review_ID is part of the DataFrame\n",
    "df = df[['Review_ID', 'User_ID', 'User_Spam_Probability', 'Review_Spam_Probability'] + \n",
    "        [col for col in df.columns if col not in ['Review_ID', 'User_ID', 'User_Spam_Probability', 'Review_Spam_Probability']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write SpEagle Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Output_Data directory exists\n",
    "output_dir = 'Output_Data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define path for the output file\n",
    "output_file_path = os.path.join(output_dir, 'SpEagle_Output.csv')\n",
    "\n",
    "# Write the complete DataFrame to a CSV file in the Output_Data folder\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
